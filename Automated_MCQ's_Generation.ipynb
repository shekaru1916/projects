{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mVDlfAhxw8K",
        "outputId": "c7d8eee3-85b6-40d8-be02-0d70cb753471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 6.81 s (started: 2024-07-11 10:00:54 +00:00)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO3PoxPPprVG",
        "outputId": "99e8d093-27b7-49b3-c934-6b2c639812c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-12rhwzfa\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-12rhwzfa\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.8.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.2.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (0.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.4.2)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (2024.5.15)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pke==2.0.0) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2.3->pke==2.0.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (0.1.2)\n",
            "time: 14.6 s (started: 2024-07-11 10:01:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet flashtext==2.7\n",
        "!pip install git+https://github.com/boudinfl/pke.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWYN3eSjp8y5",
        "outputId": "f000d474-5720-46cb-850e-aa34a0d05b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.4.0 which is incompatible.\n",
            "tokenizers 0.21.1 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.4.0 which is incompatible.\n",
            "accelerate 1.5.2 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.4.0 which is incompatible.\n",
            "transformers 4.51.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 0.4.0 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.4.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.7/508.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentencepiece (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet huggingface-hub==0.4.0\n",
        "!pip install --quiet transformers==4.8.1\n",
        "!pip install --quiet sentencepiece==0.1.95\n",
        "!pip install --quiet textwrap3==0.9.2\n",
        "!pip install --quiet gradio==3.0.20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7SXDJmTqAqt",
        "outputId": "0033fb89-e534-4d04-814e-3463f5c5c25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 11.4 s (started: 2024-07-11 10:01:49 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet strsim==0.0.3\n",
        "!pip install --quiet sense2vec==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psnzc5k9qMIb",
        "outputId": "79ce01ed-e202-467d-e5d1-c151c70965c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 4.92 s (started: 2024-07-11 10:02:00 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0RbHn3BqQCJ",
        "outputId": "ae8482c0-91cb-46da-bc2f-9c1488790d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 6.52 s (started: 2024-07-11 10:02:05 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet sentence-transformers==2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcQy1RNC_4Xb"
      },
      "source": [
        "Video to text coversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm8ydSNK_20t",
        "outputId": "58b4b9be-cc6a-4b71-f332-3a9ccc14ff86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.4)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.6.2)\n",
            "time: 5.22 s (started: 2024-07-11 10:02:12 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YIiGp4deABOi",
        "outputId": "f2961455-679f-4bd5-b24f-34bbf809b933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Writing audio in converted.wav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "ename": "RequestError",
          "evalue": "recognition connection failed: [Errno 32] Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1349\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1076\u001b[0m                         \u001b[0;34m+\u001b[0m \u001b[0;34mb'\\r\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speech_recognition/recognizers/google.py\u001b[0m in \u001b[0;36mobtain_transcription\u001b[0;34m(request, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 32] Broken pipe>",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRequestError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c380476b0da6>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize_google\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Step 4: Finally exporting the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speech_recognition/recognizers/google.py\u001b[0m in \u001b[0;36mrecognize_legacy\u001b[0;34m(recognizer, audio_data, key, language, pfilter, show_all, with_confidence, endpoint)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     response_text = obtain_transcription(\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperation_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speech_recognition/recognizers/google.py\u001b[0m in \u001b[0;36mobtain_transcription\u001b[0;34m(request, timeout)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRequestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recognition request failed: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         raise RequestError(\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0;34m\"recognition connection failed: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         )\n",
            "\u001b[0;31mRequestError\u001b[0m: recognition connection failed: [Errno 32] Broken pipe"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 16.7 s (started: 2024-07-11 10:04:33 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Step 1 : Importing libararies\n",
        "import speech_recognition as sr\n",
        "import moviepy.editor as mp\n",
        "\n",
        "# Step 2: Video to Audio conversion\n",
        "\n",
        "VidClip = mp.VideoFileClip(\"/content/videoplayback.mp4\")\n",
        "VidClip.audio.write_audiofile(\"converted.wav\")\n",
        "\n",
        "# Step 3: Speech recognition\n",
        "\n",
        "reco = sr.Recognizer()\n",
        "audio = sr.AudioFile(\"converted.wav\")\n",
        "with audio as source:\n",
        "  audio_file = reco.record(source)\n",
        "result = reco.recognize_google(audio_file)\n",
        "\n",
        "# Step 4: Finally exporting the result\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ctWCB0qeYD"
      },
      "source": [
        "**Example 1**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZXg936pfla-"
      },
      "outputs": [],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDIqlRejqkgK"
      },
      "source": [
        "Text taken from: https://gadgets.ndtv.com/internet/news/dogecoin-price-rally-surge-elon-musk-tweet-twitter-working-developers-improve-transaction-efficiency-2442120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6pa_qBvqa-k"
      },
      "outputs": [],
      "source": [
        "from textwrap3 import wrap\n",
        "\n",
        "text = \"\"\"Elon Musk has shown again he can influence the digital currency market with just his tweets. After saying that his electric vehicle-making company\n",
        "Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve\n",
        "system transaction efficiency. Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin\n",
        "rallied by about 20 percent. The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  In a recent tweet,\n",
        "Musk put out a statement from Tesla that it was “concerned” about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and\n",
        "transaction, and hence was suspending vehicle purchases using the cryptocurrency.  A day later he again tweeted saying, “To be clear, I strongly\n",
        "believe in crypto, but it can't drive a massive increase in fossil fuel use, especially coal”.  It triggered a downward spiral for Bitcoin value but\n",
        "the cryptocurrency has stabilised since.   A number of Twitter users welcomed Musk's statement. One of them said it's time people started realising\n",
        "that Dogecoin “is here to stay” and another referred to Musk's previous assertion that crypto could become the world's future currency.\"\"\"\n",
        "\n",
        "for wrp in wrap(text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgWGSPMCq4Yc"
      },
      "source": [
        "**Example2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zfgf5-3dqrjV"
      },
      "outputs": [],
      "source": [
        "from textwrap3 import wrap\n",
        "for wrp in wrap(result, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot1YNJx8rBVg"
      },
      "source": [
        "**Summarization with T5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfIkmQr5rAQB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "summary_model = summary_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBMRlSYerJa3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoFgBrfLrN8z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def postprocesstext (content):\n",
        "  final=\"\"\n",
        "  for sent in sent_tokenize(content):\n",
        "    sent = sent.capitalize()\n",
        "    final = final +\" \"+sent\n",
        "  return final\n",
        "\n",
        "\n",
        "def summarizer(text,model,tokenizer):\n",
        "  text = text.strip().replace(\"\\n\",\" \")\n",
        "  text = \"summarize: \"+text\n",
        "  # print (text)\n",
        "  max_len = 512\n",
        "  encoding = tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=3,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  min_length = 75,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  summary = dec[0]\n",
        "  summary = postprocesstext(summary)\n",
        "  summary= summary.strip()\n",
        "\n",
        "  return summary\n",
        "\n",
        "\n",
        "summarized_text = summarizer(result,summary_model,summary_tokenizer)\n",
        "\n",
        "\n",
        "print (\"\\noriginal Text >>\")\n",
        "for wrp in wrap(result, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n",
        "print (\"Summarized Text >>\")\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulTZYf3hrXr7"
      },
      "source": [
        "**Answer Span Extraction (Keywords and Noun Phrases)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmHziMTLrTKt"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pke\n",
        "import traceback\n",
        "\n",
        "def get_nouns_multipartite(content):\n",
        "    out=[]\n",
        "    try:\n",
        "        extractor = pke.unsupervised.MultipartiteRank()\n",
        "        extractor.load_document(input=content,language='en')\n",
        "        #    not contain punctuation marks or stopwords as candidates.\n",
        "        pos = {'PROPN','NOUN'}\n",
        "        #pos = {'PROPN','NOUN'}\n",
        "        stoplist = list(string.punctuation)\n",
        "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "        stoplist += stopwords.words('english')\n",
        "        # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
        "        extractor.candidate_selection(pos=pos)\n",
        "        # 4. build the Multipartite graph and rank candidates using random walk,\n",
        "        #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
        "        #    threshold/method parameters.\n",
        "        extractor.candidate_weighting(alpha=1.1,\n",
        "                                      threshold=0.75,\n",
        "                                      method='average')\n",
        "        keyphrases = extractor.get_n_best(n=15)\n",
        "\n",
        "\n",
        "        for val in keyphrases:\n",
        "            out.append(val[0])\n",
        "    except:\n",
        "        out = []\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzdoWkTHrl1O"
      },
      "outputs": [],
      "source": [
        "from flashtext import KeywordProcessor\n",
        "\n",
        "\n",
        "def get_keywords(originaltext,summarytext):\n",
        "  keywords = get_nouns_multipartite(originaltext)\n",
        "  print (\"keywords unsummarized: \",keywords)\n",
        "  keyword_processor = KeywordProcessor()\n",
        "  for keyword in keywords:\n",
        "    keyword_processor.add_keyword(keyword)\n",
        "\n",
        "  keywords_found = keyword_processor.extract_keywords(summarytext)\n",
        "  keywords_found = list(set(keywords_found))\n",
        "  print (\"keywords_found in summarized: \",keywords_found)\n",
        "\n",
        "  important_keywords =[]\n",
        "  for keyword in keywords:\n",
        "    if keyword in keywords_found:\n",
        "      important_keywords.append(keyword)\n",
        "\n",
        "  return important_keywords[:4]\n",
        "\n",
        "\n",
        "imp_keywords = get_keywords(text,summarized_text)\n",
        "print (imp_keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48TkEt4hrplp"
      },
      "outputs": [],
      "source": [
        "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_model = question_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qaz4GU_r2c1"
      },
      "outputs": [],
      "source": [
        "def get_question(context,answer,model,tokenizer):\n",
        "  text = \"context: {} answer: {}\".format(context,answer)\n",
        "  encoding = tokenizer.encode_plus(text,max_length=384, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=72)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "\n",
        "\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n",
        "\n",
        "for answer in imp_keywords:\n",
        "  ques = get_question(summarized_text,answer,question_model,question_tokenizer)\n",
        "  print (ques)\n",
        "  print (answer.capitalize())\n",
        "  print (\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeaFRDQTr7kK"
      },
      "source": [
        "Gradio UI Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWW6Cu5Qr5pW"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "context = gr.inputs.Textbox(lines=10, placeholder=\"Enter video path here...\")\n",
        "output = gr.outputs.HTML(  label=\"Question and Answers\")\n",
        "\n",
        "def generate_question(context):\n",
        "\n",
        "  VidClip = mp.VideoFileClip(context)\n",
        "  VidClip.audio.write_audiofile(\"converted.wav\")\n",
        "  reco = sr.Recognizer()\n",
        "  audio = sr.AudioFile(\"converted.wav\")\n",
        "  with audio as source:\n",
        "    audio_file = reco.record(source)\n",
        "  result = reco.recognize_google(audio_file)\n",
        "  context=result\n",
        "  summary_text = summarizer(context,summary_model,summary_tokenizer)\n",
        "  for wrp in wrap(summary_text, 150):\n",
        "    print (wrp)\n",
        "  np =  get_keywords(context,summary_text)\n",
        "  print (\"\\n\\nNoun phrases\",np)\n",
        "  output=\"\"\n",
        "  for answer in np:\n",
        "    ques = get_question(summary_text,answer,question_model,question_tokenizer)\n",
        "    # output= output + ques + \"\\n\" + \"Ans: \"+answer.capitalize() + \"\\n\\n\"\n",
        "    output = output + \"<b style='color:blue;'>\" + ques + \"</b>\"\n",
        "    output = output + \"<br>\"\n",
        "    output = output + \"<b style='color:green;'>\" + \"Ans: \" +answer.capitalize()+  \"</b>\"\n",
        "    output = output + \"<br>\"\n",
        "\n",
        "  summary =\"Summary: \"+ summary_text\n",
        "  for answer in np:\n",
        "    summary = summary.replace(answer,\"<b>\"+answer+\"</b>\")\n",
        "    summary = summary.replace(answer.capitalize(),\"<b>\"+answer.capitalize()+\"</b>\")\n",
        "  output = output + \"<p>\"+summary+\"</p>\"\n",
        "\n",
        "  return output\n",
        "\n",
        "iface = gr.Interface(\n",
        "  fn=generate_question,\n",
        "  inputs=context,\n",
        "  outputs=output)\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbDr8GJ6uvu5"
      },
      "source": [
        "**Filter keywords with Maximum marginal Relevance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAIShwh6ulBk"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQ23sRnWu_hU"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "# paraphrase-distilroberta-base-v1\n",
        "sentence_transformer_model = SentenceTransformer('msmarco-distilbert-base-v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNsEW9bMvGWL"
      },
      "outputs": [],
      "source": [
        "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
        "normalized_levenshtein = NormalizedLevenshtein()\n",
        "\n",
        "def filter_same_sense_words(original,wordlist):\n",
        "  filtered_words=[]\n",
        "  base_sense =original.split('|')[1]\n",
        "  print (base_sense)\n",
        "  for eachword in wordlist:\n",
        "    if eachword[0].split('|')[1] == base_sense:\n",
        "      filtered_words.append(eachword[0].split('|')[0].replace(\"_\", \" \").title().strip())\n",
        "  return filtered_words\n",
        "\n",
        "def get_highest_similarity_score(wordlist,wrd):\n",
        "  score=[]\n",
        "  for each in wordlist:\n",
        "    score.append(normalized_levenshtein.similarity(each.lower(),wrd.lower()))\n",
        "  return max(score)\n",
        "\n",
        "def sense2vec_get_words(word,s2v,topn,question):\n",
        "    output = []\n",
        "    print (\"word \",word)\n",
        "    try:\n",
        "      sense = s2v.get_best_sense(word, senses= [\"NOUN\", \"PERSON\",\"PRODUCT\",\"LOC\",\"ORG\",\"EVENT\",\"NORP\",\"WORK OF ART\",\"FAC\",\"GPE\",\"NUM\",\"FACILITY\"])\n",
        "      most_similar = s2v.most_similar(sense, n=topn)\n",
        "      # print (most_similar)\n",
        "      output = filter_same_sense_words(sense,most_similar)\n",
        "      print (\"Similar \",output)\n",
        "    except:\n",
        "      output =[]\n",
        "\n",
        "    threshold = 0.6\n",
        "    final=[word]\n",
        "    checklist =question.split()\n",
        "    for x in output:\n",
        "      if get_highest_similarity_score(final,x)<threshold and x not in final and x not in checklist:\n",
        "        final.append(x)\n",
        "\n",
        "    return final[1:]\n",
        "\n",
        "def mmr(doc_embedding, word_embeddings, words, top_n, lambda_param):\n",
        "\n",
        "    # Extract similarity within words, and between words and the document\n",
        "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
        "    word_similarity = cosine_similarity(word_embeddings)\n",
        "\n",
        "    # Initialize candidates and already choose best keyword/keyphrase\n",
        "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "\n",
        "    for _ in range(top_n - 1):\n",
        "        # Extract similarities within candidates and\n",
        "        # between candidates and selected keywords/phrases\n",
        "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "        # Calculate MMR\n",
        "        mmr = (lambda_param) * candidate_similarities - (1-lambda_param) * target_similarities.reshape(-1, 1)\n",
        "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "        # Update keywords & candidates\n",
        "        keywords_idx.append(mmr_idx)\n",
        "        candidates_idx.remove(mmr_idx)\n",
        "\n",
        "    return [words[idx] for idx in keywords_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7DaCxlzvJ6m"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_distractors_wordnet(word):\n",
        "    distractors=[]\n",
        "    try:\n",
        "      syn = wn.synsets(word,'n')[0]\n",
        "\n",
        "      word= word.lower()\n",
        "      orig_word = word\n",
        "      if len(word.split())>0:\n",
        "          word = word.replace(\" \",\"_\")\n",
        "      hypernym = syn.hypernyms()\n",
        "      if len(hypernym) == 0:\n",
        "          return distractors\n",
        "      for item in hypernym[0].hyponyms():\n",
        "          name = item.lemmas()[0].name()\n",
        "          #print (\"name \",name, \" word\",orig_word)\n",
        "          if name == orig_word:\n",
        "              conti7⁷ oknue\n",
        "          name = name.replace(\"_\",\" \")\n",
        "          name = \" \".join(w.capitalize() for w in name.split())\n",
        "          if name is not None and name not in distractors:\n",
        "              distractors.append(name)\n",
        "    except:\n",
        "      print (\"Wordnet distractors not found\")\n",
        "    return distractors\n",
        "\n",
        "def get_distractors (word,origsentence,sense2vecmodel,sentencemodel,top_n,lambdaval):\n",
        "  distractors = sense2vec_get_words(word,sense2vecmodel,top_n,origsentence)\n",
        "  print (\"distractors \",distractors)\n",
        "  if len(distractors) ==0:\n",
        "    return distractors\n",
        "  distractors_new = [word.capitalize()]\n",
        "  distractors_new.extend(distractors)\n",
        "  # print (\"distractors_new .. \",distractors_new)\n",
        "\n",
        "  embedding_sentence = origsentence+ \" \"+word.capitalize()\n",
        "  # embedding_sentence = word\n",
        "  keyword_embedding = sentencemodel.encode([embedding_sentence])\n",
        "  distractor_embeddings = sentencemodel.encode(distractors_new)\n",
        "\n",
        "  # filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors,4,0.7)\n",
        "  max_keywords = min(len(distractors_new),5)\n",
        "  filtered_keywords = mmr(keyword_embedding, distractor_embeddings,distractors_new,max_keywords,lambdaval)\n",
        "  # filtered_keywords = filtered_keywords[1:]\n",
        "  final = [word.capitalize()]\n",
        "  for wrd in filtered_keywords:\n",
        "    if wrd.lower() !=word.lower():\n",
        "      final.append(wrd.capitalize())\n",
        "  final = final[1:]\n",
        "  return final\n",
        "\n",
        "sent = \"What cryptocurrency did Musk rarely tweet about?\"\n",
        "keyword = \"Bitcoin\"\n",
        "\n",
        "# sent = \"What did Musk say he was working with to improve system transaction efficiency?\"\n",
        "# keyword= \"Dogecoin\"\n",
        "\n",
        "\n",
        "# sent = \"What company did Musk say would not accept bitcoin payments?\"\n",
        "# keyword= \"Tesla\"\n",
        "\n",
        "\n",
        "# sent = \"What has Musk often tweeted in support of?\"\n",
        "# keyword = \"Cryptocurrency\"\n",
        "\n",
        "print (get_distractors(keyword,sent,s2v,sentence_transformer_model,40,0.2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLnX0GIMvPWP"
      },
      "outputs": [],
      "source": [
        "get_distractors_wordnet('lion')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyTE99HTvSKJ"
      },
      "source": [
        "**Gradio Visualization with MCQs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYhGSOYHvRjL"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "videoPath = gr.inputs.Textbox(lines=10, placeholder=\"Enter video path here...\")\n",
        "output = gr.outputs.HTML(  label=\"Question and Answers\")\n",
        "radiobutton = gr.inputs.Radio([\"Wordnet\", \"Sense2Vec\"])\n",
        "\n",
        "def generate_question(videoPath,radiobutton):\n",
        "\n",
        "  VidClip = mp.VideoFileClip(videoPath)\n",
        "  VidClip.audio.write_audiofile(\"converted.wav\")\n",
        "  reco = sr.Recognizer()\n",
        "  audio = sr.AudioFile(\"converted.wav\")\n",
        "  with audio as source:\n",
        "    audio_file = reco.record(source)\n",
        "  context = reco.recognize_google(audio_file)\n",
        "  summary_text = summarizer(context,summary_model,summary_tokenizer)\n",
        "  for wrp in wrap(summary_text, 150):\n",
        "    print (wrp)\n",
        "  # np = getnounphrases(summary_text,sentence_transformer_model,3)\n",
        "  np =  get_keywords(context,summary_text)\n",
        "  print (\"\\n\\nNoun phrases\",np)\n",
        "  output=\"\"\n",
        "  for answer in np:\n",
        "    ques = get_question(summary_text,answer,question_model,question_tokenizer)\n",
        "    if radiobutton==\"Wordnet\":\n",
        "      distractors = get_distractors_wordnet(answer)\n",
        "    else:\n",
        "      distractors = get_distractors(answer.capitalize(),ques,s2v,sentence_transformer_model,40,0.2)\n",
        "    # output= output + ques + \"\\n\" + \"Ans: \"+answer.capitalize() + \"\\n\\n\"\n",
        "    output = output + \"<b style='color:blue;'>\" + ques + \"</b>\"\n",
        "    output = output + \"<br>\"\n",
        "    output = output + \"<b style='color:green;'>\" + \"Ans: \" +answer.capitalize()+  \"</b>\"+\"<br>\"\n",
        "    if len(distractors)>0:\n",
        "      for distractor in distractors[:4]:\n",
        "        output = output + \"<b style='color:brown;'>\" + distractor+  \"</b>\"+\"<br>\"\n",
        "    output = output + \"<br>\"\n",
        "\n",
        "  summary =\"Summary: \"+ summary_text\n",
        "  for answer in np:\n",
        "    summary = summary.replace(answer,\"<b>\"+answer+\"</b>\" + \"<br>\")\n",
        "    summary = summary.replace(answer.capitalize(),\"<b>\"+answer.capitalize()+\"</b>\")\n",
        "  output = output + \"<p>\"+summary+\"</p>\"\n",
        "  output = output + \"<br>\"\n",
        "  return output\n",
        "iface = gr.Interface(\n",
        "  fn=generate_question,\n",
        "  inputs=[videoPath,radiobutton],\n",
        "  outputs=output)\n",
        "iface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}